{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af9005b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "import plotly.graph_objs as plotly  # for interactive plots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70656f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_reduced.pkl', 'rb') as handle:\n",
    "    X_train_reduced = pickle.load(handle)\n",
    "with open('X_val_reduced.pkl', 'rb') as handle:\n",
    "    X_val_reduced = pickle.load(handle)\n",
    "with open('X_test_reduced.pkl', 'rb') as handle:\n",
    "    X_test_reduced = pickle.load(handle)\n",
    "    \n",
    "with open(\"y_train.pkl\", 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "with open(\"y_val.pkl\", 'rb') as handle:\n",
    "    y_val = pickle.load(handle)\n",
    "with open(\"y_test.pkl\", 'rb') as handle:\n",
    "    y_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81f43f",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a9a7dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lr_model(X_train, y_train, learning_rate=0.01, random_state = 1234, epochs = 100):\n",
    "    \"\"\"Build a TF logistic regression model using Keras.\n",
    "\n",
    "    Inputs:\n",
    "        X_train (array): training data to train logistic regression model, \n",
    "                        should be doc2vec embedding \n",
    "        y_train (array): test data to train logistic regression model\n",
    "        learning_rate: The desired learning rate for Adam.\n",
    "        epochs (int): number of training iterations \n",
    "        random_state (int): set random state for reproducibility\n",
    "    Outputs:\n",
    "        model (tensorflow model): A tensorflow keras\n",
    "        history (tensorflow history): training history \n",
    " \n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    np.random.seed(random_state)\n",
    "    tf.random.set_seed(random_state)\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) \n",
    "    \n",
    "    # Build a model using keras.Sequential.\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Keras layers can do pre-processing. This layer will take 100x100 embeddings\n",
    "    # and flatten them into vectors of size 10000.\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # This layer constructs the linear set of parameters for each input feature\n",
    "    # (as well as a bias), and applies a sigmoid to the result. The result is\n",
    "    # binary logistic regression.\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=1,                     # output dim (for binary classification)\n",
    "      use_bias=True,               # use a bias param\n",
    "      activation=\"sigmoid\"         # apply the sigmoid function!\n",
    "    ))\n",
    "\n",
    "    # Use the Adam optimizer.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                optimizer=optimizer, \n",
    "                metrics=[\"accuracy\"])\n",
    "    history = model.fit(\n",
    "        x = X_train,\n",
    "        y = y_train,\n",
    "        validation_split=0.2, \n",
    "        epochs=epochs, \n",
    "        callbacks=[callback])\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7600194",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f722079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "X_train_sent_embedded_scaled = sc_x.fit(X_train_sent_embedded).transform(X_train_sent_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b58eb5fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Learning Rate: 0.0001\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.6092 - accuracy: 0.6998 - val_loss: 0.5056 - val_accuracy: 0.7627\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.4593 - accuracy: 0.7929 - val_loss: 0.4304 - val_accuracy: 0.8109\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.4087 - accuracy: 0.8253 - val_loss: 0.3997 - val_accuracy: 0.8302\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3865 - accuracy: 0.8394 - val_loss: 0.3845 - val_accuracy: 0.8400\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3749 - accuracy: 0.8456 - val_loss: 0.3760 - val_accuracy: 0.8460\n",
      "Epoch 6/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3682 - accuracy: 0.8500 - val_loss: 0.3707 - val_accuracy: 0.8480\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3638 - accuracy: 0.8517 - val_loss: 0.3671 - val_accuracy: 0.8501\n",
      "Epoch 8/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3607 - accuracy: 0.8530 - val_loss: 0.3646 - val_accuracy: 0.8515\n",
      "Epoch 9/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3586 - accuracy: 0.8543 - val_loss: 0.3630 - val_accuracy: 0.8534\n",
      "Epoch 10/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3571 - accuracy: 0.8547 - val_loss: 0.3616 - val_accuracy: 0.8539\n",
      "Epoch 11/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3559 - accuracy: 0.8557 - val_loss: 0.3606 - val_accuracy: 0.8538\n",
      "Epoch 12/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3550 - accuracy: 0.8561 - val_loss: 0.3598 - val_accuracy: 0.8547\n",
      "Epoch 13/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3542 - accuracy: 0.8568 - val_loss: 0.3593 - val_accuracy: 0.8548\n",
      "Epoch 14/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3537 - accuracy: 0.8570 - val_loss: 0.3588 - val_accuracy: 0.8548\n",
      "Epoch 15/100\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3533 - accuracy: 0.8570 - val_loss: 0.3584 - val_accuracy: 0.8549\n",
      "Epoch 16/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3529 - accuracy: 0.8573 - val_loss: 0.3582 - val_accuracy: 0.8558\n",
      "Epoch 17/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3526 - accuracy: 0.8575 - val_loss: 0.3579 - val_accuracy: 0.8558\n",
      "Epoch 18/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3524 - accuracy: 0.8575 - val_loss: 0.3578 - val_accuracy: 0.8563\n",
      "Epoch 19/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3522 - accuracy: 0.8576 - val_loss: 0.3576 - val_accuracy: 0.8560\n",
      "Epoch 20/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3520 - accuracy: 0.8575 - val_loss: 0.3574 - val_accuracy: 0.8561\n",
      "Epoch 21/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3519 - accuracy: 0.8576 - val_loss: 0.3573 - val_accuracy: 0.8562\n",
      "Epoch 22/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3518 - accuracy: 0.8581 - val_loss: 0.3573 - val_accuracy: 0.8557\n",
      "Epoch 23/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3518 - accuracy: 0.8578 - val_loss: 0.3572 - val_accuracy: 0.8560\n",
      "Epoch 24/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3517 - accuracy: 0.8579 - val_loss: 0.3572 - val_accuracy: 0.8558\n",
      "Epoch 25/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3516 - accuracy: 0.8582 - val_loss: 0.3571 - val_accuracy: 0.8557\n",
      "Epoch 26/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3516 - accuracy: 0.8581 - val_loss: 0.3571 - val_accuracy: 0.8557\n",
      "Epoch 27/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3515 - accuracy: 0.8581 - val_loss: 0.3571 - val_accuracy: 0.8559\n",
      "Epoch 28/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3515 - accuracy: 0.8580 - val_loss: 0.3570 - val_accuracy: 0.8558\n",
      "Epoch 29/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3515 - accuracy: 0.8581 - val_loss: 0.3570 - val_accuracy: 0.8559\n",
      "Epoch 30/100\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3515 - accuracy: 0.8584 - val_loss: 0.3570 - val_accuracy: 0.8560\n",
      "Epoch 31/100\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.3514 - accuracy: 0.8584 - val_loss: 0.3570 - val_accuracy: 0.8557\n",
      "Epoch 32/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3514 - accuracy: 0.8583 - val_loss: 0.3570 - val_accuracy: 0.8560\n",
      "Epoch 33/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3514 - accuracy: 0.8584 - val_loss: 0.3570 - val_accuracy: 0.8556\n",
      "Epoch 34/100\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.3514 - accuracy: 0.8585 - val_loss: 0.3570 - val_accuracy: 0.8557\n",
      "Epoch 35/100\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.3514 - accuracy: 0.8583 - val_loss: 0.3570 - val_accuracy: 0.8558\n",
      "Epoch 36/100\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3514 - accuracy: 0.8584 - val_loss: 0.3570 - val_accuracy: 0.8555\n",
      "Epoch 37/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3514 - accuracy: 0.8580 - val_loss: 0.3570 - val_accuracy: 0.8561\n",
      "\n",
      "Training Learning Rate: 0.001\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.4031 - accuracy: 0.8280 - val_loss: 0.3636 - val_accuracy: 0.8507\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3556 - accuracy: 0.8558 - val_loss: 0.3580 - val_accuracy: 0.8548\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3533 - accuracy: 0.8571 - val_loss: 0.3575 - val_accuracy: 0.8562\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3530 - accuracy: 0.8576 - val_loss: 0.3577 - val_accuracy: 0.8557\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3530 - accuracy: 0.8579 - val_loss: 0.3583 - val_accuracy: 0.8569\n",
      "Epoch 6/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3530 - accuracy: 0.8578 - val_loss: 0.3577 - val_accuracy: 0.8581\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3531 - accuracy: 0.8575 - val_loss: 0.3575 - val_accuracy: 0.8569\n",
      "Epoch 8/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3530 - accuracy: 0.8581 - val_loss: 0.3569 - val_accuracy: 0.8572\n",
      "Epoch 9/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3528 - accuracy: 0.8576 - val_loss: 0.3588 - val_accuracy: 0.8569\n",
      "Epoch 10/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3530 - accuracy: 0.8577 - val_loss: 0.3579 - val_accuracy: 0.8573\n",
      "Epoch 11/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3531 - accuracy: 0.8578 - val_loss: 0.3578 - val_accuracy: 0.8557\n",
      "Epoch 12/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3529 - accuracy: 0.8575 - val_loss: 0.3579 - val_accuracy: 0.8568\n",
      "Epoch 13/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3529 - accuracy: 0.8577 - val_loss: 0.3584 - val_accuracy: 0.8571\n",
      "\n",
      "Training Learning Rate: 0.01\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3710 - accuracy: 0.8499 - val_loss: 0.3710 - val_accuracy: 0.8490\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3669 - accuracy: 0.8524 - val_loss: 0.3682 - val_accuracy: 0.8518\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3662 - accuracy: 0.8525 - val_loss: 0.3702 - val_accuracy: 0.8498\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3665 - accuracy: 0.8524 - val_loss: 0.3718 - val_accuracy: 0.8480\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3661 - accuracy: 0.8527 - val_loss: 0.3713 - val_accuracy: 0.8484\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3666 - accuracy: 0.8530 - val_loss: 0.3687 - val_accuracy: 0.8564\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3671 - accuracy: 0.8525 - val_loss: 0.3704 - val_accuracy: 0.8501\n",
      "\n",
      "Training Learning Rate: 0.1\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.5439 - accuracy: 0.8214 - val_loss: 0.6034 - val_accuracy: 0.8214\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.5552 - accuracy: 0.8196 - val_loss: 0.5162 - val_accuracy: 0.8321\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.5548 - accuracy: 0.8201 - val_loss: 0.5473 - val_accuracy: 0.8156\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.5568 - accuracy: 0.8201 - val_loss: 0.5735 - val_accuracy: 0.8107\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.5545 - accuracy: 0.8199 - val_loss: 0.5584 - val_accuracy: 0.8202\n",
      "Epoch 6/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.5571 - accuracy: 0.8215 - val_loss: 0.5423 - val_accuracy: 0.8261\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.5547 - accuracy: 0.8211 - val_loss: 0.5473 - val_accuracy: 0.8193\n",
      "\n",
      "Training Learning Rate: 1\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.4754 - accuracy: 0.7999 - val_loss: 3.7641 - val_accuracy: 0.7991\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6337 - accuracy: 0.7973 - val_loss: 3.9531 - val_accuracy: 0.7685\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6776 - accuracy: 0.7981 - val_loss: 3.5807 - val_accuracy: 0.7812\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6679 - accuracy: 0.7991 - val_loss: 4.1543 - val_accuracy: 0.7838\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6256 - accuracy: 0.7974 - val_loss: 3.9348 - val_accuracy: 0.7914\n",
      "Epoch 6/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6839 - accuracy: 0.7988 - val_loss: 3.3442 - val_accuracy: 0.7923\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.5957 - accuracy: 0.7997 - val_loss: 3.4662 - val_accuracy: 0.8136\n",
      "Epoch 8/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6807 - accuracy: 0.7968 - val_loss: 3.7651 - val_accuracy: 0.7971\n",
      "Epoch 9/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6840 - accuracy: 0.7976 - val_loss: 3.5552 - val_accuracy: 0.8205\n",
      "Epoch 10/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.5577 - accuracy: 0.7983 - val_loss: 3.1119 - val_accuracy: 0.8118\n",
      "Epoch 11/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.5934 - accuracy: 0.7969 - val_loss: 3.8631 - val_accuracy: 0.8119\n",
      "Epoch 12/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6701 - accuracy: 0.7966 - val_loss: 4.2842 - val_accuracy: 0.7556\n",
      "Epoch 13/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6434 - accuracy: 0.7979 - val_loss: 3.0254 - val_accuracy: 0.7962\n",
      "Epoch 14/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.7550 - accuracy: 0.7967 - val_loss: 3.8635 - val_accuracy: 0.8176\n",
      "Epoch 15/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6099 - accuracy: 0.7984 - val_loss: 5.7810 - val_accuracy: 0.7067\n",
      "Epoch 16/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.7631 - accuracy: 0.8000 - val_loss: 3.7153 - val_accuracy: 0.7732\n",
      "Epoch 17/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 3.6594 - accuracy: 0.7960 - val_loss: 3.2531 - val_accuracy: 0.7921\n",
      "Epoch 18/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 3.6740 - accuracy: 0.7976 - val_loss: 3.4922 - val_accuracy: 0.8115\n"
     ]
    }
   ],
   "source": [
    "models_d2v = []\n",
    "histories_d2v = []\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for lr in learning_rates: \n",
    "    print(f\"\\nTraining Learning Rate: {lr}\")\n",
    "    model, history = build_lr_model(X_train_sent_embedded_scaled, y_train, learning_rate = lr)\n",
    "    models_d2v.append(model)\n",
    "    histories_d2v.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a97d4b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v_lr = models_d2v[1]\n",
    "model_d2v_lr.save(\"model_d2v_lr.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e9b0804c",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def build_nn(X_train, y_train, hidden_layers = [], epochs = 20, random_state = 1234):\n",
    "    '''\n",
    "    Function to generalize neural network training\n",
    "    \n",
    "    Inputs:\n",
    "        X_train (array): training data to train model, should doc2vec embedding \n",
    "        y_train (array): test data to train model\n",
    "        hidden_layer (list): list of nodes to use for each new layer in neural network \n",
    "        epochs (int): number of training iterations \n",
    "        random_state (int): set random state for reproducibility\n",
    "    Outputs:\n",
    "        model (tensorflow model): A tensorflow keras\n",
    "        history (tensorflow history): training history \n",
    "    \n",
    "    '''\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(random_state)\n",
    "    #Add early stopping to prevent extreme overfitting\n",
    "    #If validation loss hasn't improved in 3 iterations, stop training \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) \n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #Add more layers to neural network, each input in hidden_layer is additional layer \n",
    "    for nodes in hidden_layers:\n",
    "        model.add(tf.keras.layers.Dense(nodes, activation = \"relu\"))\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "    model.compile(loss = \"binary_crossentropy\",\n",
    "             optimizer = \"adam\", \n",
    "             metrics = [\"accuracy\"])\n",
    "    \n",
    "    #Fit the training and test dataset \n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    history = model.fit(\n",
    "        x = X_train,\n",
    "        y = y_train,\n",
    "        validation_split=0.2, \n",
    "        epochs=epochs, \n",
    "        callbacks=[callback])\n",
    "    \n",
    "    return model, history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "be0ea790",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\n",
      "Epoch 1/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.3564 - accuracy: 0.8495 - val_loss: 0.3023 - val_accuracy: 0.8780\n",
      "Epoch 2/20\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3049 - accuracy: 0.8767 - val_loss: 0.2922 - val_accuracy: 0.8811\n",
      "Epoch 3/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2962 - accuracy: 0.8802 - val_loss: 0.2887 - val_accuracy: 0.8818\n",
      "Epoch 4/20\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.2926 - accuracy: 0.8817 - val_loss: 0.2894 - val_accuracy: 0.8819\n",
      "Epoch 5/20\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.2908 - accuracy: 0.8813 - val_loss: 0.2859 - val_accuracy: 0.8830\n",
      "Epoch 6/20\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.2874 - accuracy: 0.8828 - val_loss: 0.2875 - val_accuracy: 0.8834\n",
      "Epoch 7/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2862 - accuracy: 0.8839 - val_loss: 0.2864 - val_accuracy: 0.8819\n",
      "Epoch 8/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2848 - accuracy: 0.8840 - val_loss: 0.2838 - val_accuracy: 0.8850\n",
      "Epoch 9/20\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.2840 - accuracy: 0.8857 - val_loss: 0.2840 - val_accuracy: 0.8827\n",
      "Epoch 10/20\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.2836 - accuracy: 0.8855 - val_loss: 0.2845 - val_accuracy: 0.8842\n",
      "Epoch 11/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2838 - accuracy: 0.8854 - val_loss: 0.2853 - val_accuracy: 0.8836\n",
      "Epoch 12/20\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.2821 - accuracy: 0.8846 - val_loss: 0.2864 - val_accuracy: 0.8820\n",
      "Epoch 13/20\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.2802 - accuracy: 0.8859 - val_loss: 0.2887 - val_accuracy: 0.8837\n",
      "[100, 50]\n",
      "Epoch 1/20\n",
      "3482/3482 [==============================] - 11s 3ms/step - loss: 0.3546 - accuracy: 0.8496 - val_loss: 0.2857 - val_accuracy: 0.8820\n",
      "Epoch 2/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.3030 - accuracy: 0.8768 - val_loss: 0.2865 - val_accuracy: 0.8774\n",
      "Epoch 3/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.2939 - accuracy: 0.8801 - val_loss: 0.2776 - val_accuracy: 0.8850\n",
      "Epoch 4/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2895 - accuracy: 0.8824 - val_loss: 0.2792 - val_accuracy: 0.8819\n",
      "Epoch 5/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.2876 - accuracy: 0.8833 - val_loss: 0.2765 - val_accuracy: 0.8848\n",
      "Epoch 6/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.2840 - accuracy: 0.8840 - val_loss: 0.2758 - val_accuracy: 0.8832\n",
      "Epoch 7/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.2829 - accuracy: 0.8861 - val_loss: 0.2808 - val_accuracy: 0.8836\n",
      "Epoch 8/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2810 - accuracy: 0.8868 - val_loss: 0.2723 - val_accuracy: 0.8862\n",
      "Epoch 9/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2798 - accuracy: 0.8875 - val_loss: 0.2739 - val_accuracy: 0.8851\n",
      "Epoch 10/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2791 - accuracy: 0.8878 - val_loss: 0.2745 - val_accuracy: 0.8855\n",
      "Epoch 11/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2777 - accuracy: 0.8878 - val_loss: 0.2733 - val_accuracy: 0.8847\n",
      "Epoch 12/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2760 - accuracy: 0.8886 - val_loss: 0.2740 - val_accuracy: 0.8850\n",
      "Epoch 13/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.2758 - accuracy: 0.8882 - val_loss: 0.2789 - val_accuracy: 0.8836\n",
      "[50, 25]\n",
      "Epoch 1/20\n",
      "3482/3482 [==============================] - 11s 3ms/step - loss: 0.3879 - accuracy: 0.8341 - val_loss: 0.2944 - val_accuracy: 0.8810\n",
      "Epoch 2/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3214 - accuracy: 0.8691 - val_loss: 0.2851 - val_accuracy: 0.8820\n",
      "Epoch 3/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3105 - accuracy: 0.8758 - val_loss: 0.2828 - val_accuracy: 0.8823\n",
      "Epoch 4/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3065 - accuracy: 0.8774 - val_loss: 0.2823 - val_accuracy: 0.8815\n",
      "Epoch 5/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3038 - accuracy: 0.8783 - val_loss: 0.2811 - val_accuracy: 0.8829\n",
      "Epoch 6/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3011 - accuracy: 0.8796 - val_loss: 0.2865 - val_accuracy: 0.8776\n",
      "Epoch 7/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.2981 - accuracy: 0.8813 - val_loss: 0.2791 - val_accuracy: 0.8843\n",
      "Epoch 8/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.2980 - accuracy: 0.8819 - val_loss: 0.2810 - val_accuracy: 0.8809\n",
      "Epoch 9/20\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.2972 - accuracy: 0.8820 - val_loss: 0.2815 - val_accuracy: 0.8818\n",
      "Epoch 10/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2964 - accuracy: 0.8827 - val_loss: 0.2812 - val_accuracy: 0.8815\n",
      "Epoch 11/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2950 - accuracy: 0.8822 - val_loss: 0.2773 - val_accuracy: 0.8850\n",
      "Epoch 12/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2928 - accuracy: 0.8832 - val_loss: 0.2775 - val_accuracy: 0.8850\n",
      "Epoch 13/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2941 - accuracy: 0.8818 - val_loss: 0.2805 - val_accuracy: 0.8824\n",
      "Epoch 14/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2917 - accuracy: 0.8832 - val_loss: 0.2788 - val_accuracy: 0.8836\n",
      "Epoch 15/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2923 - accuracy: 0.8837 - val_loss: 0.2792 - val_accuracy: 0.8837\n",
      "Epoch 16/20\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.2912 - accuracy: 0.8845 - val_loss: 0.2774 - val_accuracy: 0.8839\n",
      "[100, 50, 25]\n",
      "Epoch 1/20\n",
      "3482/3482 [==============================] - 11s 3ms/step - loss: 0.3790 - accuracy: 0.8377 - val_loss: 0.2903 - val_accuracy: 0.8800\n",
      "Epoch 2/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.3185 - accuracy: 0.8734 - val_loss: 0.2834 - val_accuracy: 0.8813\n",
      "Epoch 3/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.3077 - accuracy: 0.8769 - val_loss: 0.2807 - val_accuracy: 0.8838\n",
      "Epoch 4/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.3048 - accuracy: 0.8785 - val_loss: 0.2843 - val_accuracy: 0.8811\n",
      "Epoch 5/20\n",
      "3482/3482 [==============================] - 11s 3ms/step - loss: 0.3009 - accuracy: 0.8811 - val_loss: 0.2793 - val_accuracy: 0.8837\n",
      "Epoch 6/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.3001 - accuracy: 0.8811 - val_loss: 0.2832 - val_accuracy: 0.8820\n",
      "Epoch 7/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2943 - accuracy: 0.8841 - val_loss: 0.2784 - val_accuracy: 0.8849\n",
      "Epoch 8/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2941 - accuracy: 0.8842 - val_loss: 0.2771 - val_accuracy: 0.8853\n",
      "Epoch 9/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2928 - accuracy: 0.8850 - val_loss: 0.2774 - val_accuracy: 0.8842\n",
      "Epoch 10/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2909 - accuracy: 0.8853 - val_loss: 0.2814 - val_accuracy: 0.8848\n",
      "Epoch 11/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2896 - accuracy: 0.8856 - val_loss: 0.2826 - val_accuracy: 0.8844\n",
      "Epoch 12/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2891 - accuracy: 0.8857 - val_loss: 0.2802 - val_accuracy: 0.8841\n",
      "Epoch 13/20\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 0.2894 - accuracy: 0.8856 - val_loss: 0.2819 - val_accuracy: 0.8848\n"
     ]
    }
   ],
   "source": [
    "d2v_models_nn = []\n",
    "d2v_histories_nn = [] \n",
    "hidden_layers = [[100], [100, 50], [50, 25], [100, 50, 25]]\n",
    "for hidden_layer in hidden_layers: \n",
    "    print(hidden_layer)\n",
    "    model, history = build_nn(X_train_sent_embedded, y_train, hidden_layers = hidden_layer)\n",
    "    d2v_models_nn.append(model)\n",
    "    d2v_histories_nn.append(history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "75de1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v_nn = d2v_models_nn[1]\n",
    "\n",
    "model_d2v_nn.save(\"model_d2v_nn.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('models_d2v.pkl', 'wb') as handle:\n",
    "#     pickle.dump(models_d2v, handle)\n",
    "# with open('histories_d2v.pkl', 'wb') as handle:\n",
    "#     pickle.dump(histories_d2v, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430fade",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a008b4",
   "metadata": {},
   "source": [
    "Unable to incorporate tensorflow random forest/boosted trees model with tensorflow Sequential model, use LightGBM for runtime. \n",
    "Light GBM is different than traditional Gradient Boosted Trees in that it grows leaf first instead of level first, allowing it to train much faster.\n",
    "\n",
    "References: \n",
    "- https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n",
    "\n",
    "- https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html#deal-with-over-fitting\n",
    "\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/lightgbm-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02a952",
   "metadata": {},
   "source": [
    "## Avg Word Embeddings and Doc2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107c255",
   "metadata": {},
   "source": [
    "Word embeddings are 2D arrays, take the average of the embeddings across each embedding index to flatten into 1D array, concatenate with 1D Doc2Vec embeddings in case DocwVec embeddings capture some information not already captured in the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0ffcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original word embeddings\n",
    "with open('X_train_word_embedded.pkl', 'rb') as handle:\n",
    "    X_train_word_embedded = pickle.load(handle)\n",
    "with open('X_val_word_embedded.pkl', 'rb') as handle:\n",
    "    X_val_word_embedded = pickle.load(handle)\n",
    "with open('X_test_word_embedded.pkl', 'rb') as handle:\n",
    "    X_test_word_embedded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc5fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D CNN word embeddings\n",
    "with open('X_train_cnn_word_embedded.pkl', 'rb') as handle:\n",
    "    X_train_cnn_word_embedded = pickle.load(handle)\n",
    "with open('X_val_cnn_word_embedded.pkl', 'rb') as handle:\n",
    "    X_val_cnn_word_embedded = pickle.load(handle)\n",
    "with open('X_test_cnn_word_embedded.pkl', 'rb') as handle:\n",
    "    X_test_cnn_word_embedded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7f90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec embeddings\n",
    "with open('d2v_train.pkl', 'rb') as handle:\n",
    "    X_train_sent_embedded = pickle.load(handle)\n",
    "with open('d2v_val.pkl', 'rb') as handle:\n",
    "    X_val_sent_embedded = pickle.load(handle)\n",
    "with open('d2v_test.pkl', 'rb') as handle:\n",
    "    X_test_sent_embedded = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2c3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_train_word_embedded])\n",
    "X_val_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_val_word_embedded])\n",
    "X_test_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_test_word_embedded])\n",
    "\n",
    "X_train_concatenated = np.concatenate((X_train_word_embedded_avg, X_train_sent_embedded), axis = 1)\n",
    "X_val_concatenated = np.concatenate((X_val_word_embedded_avg, X_val_sent_embedded), axis = 1)\n",
    "X_test_concatenated = np.concatenate((X_test_word_embedded_avg, X_test_sent_embedded), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ddf6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_train_cnn_word_embedded])\n",
    "X_val_cnn_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_val_cnn_word_embedded])\n",
    "X_test_cnn_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_test_cnn_word_embedded])\n",
    "\n",
    "X_train_cnn_concatenated = np.concatenate((X_train_cnn_word_embedded_avg, X_train_sent_embedded), axis = 1)\n",
    "X_val_cnn_concatenated = np.concatenate((X_val_cnn_word_embedded_avg, X_val_sent_embedded), axis = 1)\n",
    "X_test_cnn_concatenated = np.concatenate((X_test_cnn_word_embedded_avg, X_test_sent_embedded), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e696aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_concatenated = np.concatenate((X_train_word_embedded_avg, X_train_cnn_word_embedded_avg, X_train_sent_embedded), axis = 1)\n",
    "X_val_all_concatenated = np.concatenate((X_val_word_embedded_avg, X_val_cnn_word_embedded_avg, X_val_sent_embedded), axis = 1)\n",
    "X_test_all_concatenated = np.concatenate((X_test_word_embedded_avg, X_test_cnn_word_embedded_avg, X_test_sent_embedded), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359fec30",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fced9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [2000] #Set to large number, set early stopping rounds\n",
    "learning_rate = [0.01] \n",
    "num_leaves = [15, 30, 50, 70] #tree grows leaf wise instead of depth wise for lightgbm, focus on tuning num leaves\n",
    "min_child_samples = [100] #set number of entries needed in a leaf to prevent overfitting, set to 100 since data is large\n",
    "reg_alpha = [0.1, 0.2, 0.3] #combined averaged word2vec embeddings with doc2vec embedding, so could be redundancy, set L1 regularization to drop out unnecessary features\n",
    "early_stopping_rounds = [50]\n",
    "random_state = [42]\n",
    "\n",
    "param_grid = {\"n_estimators\": n_estimators,\n",
    "              \"learning_rate\": learning_rate,\n",
    "              \"num_leaves\": num_leaves,\n",
    "              \"min_child_samples\": min_child_samples,\n",
    "              \"reg_alpha\": reg_alpha,\n",
    "              \"early_stopping_rounds\": early_stopping_rounds,\n",
    "              \"random_state\": random_state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "705de443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 69639, number of negative: 69604\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 139243, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[446]\tvalid_0's binary_logloss: 0.204129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'early_stopping_rounds': 50,\n",
       " 'learning_rate': 0.01,\n",
       " 'min_child_samples': 100,\n",
       " 'n_estimators': 2000,\n",
       " 'num_leaves': 50,\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': 0.3}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier()\n",
    "grid_search = GridSearchCV(estimator = lgb_model, param_grid = param_grid, cv = 5,\n",
    "                          n_jobs = -1, verbose = 2, scoring = \"f1\") \n",
    "grid_search.fit(X_train_concatenated, y_train, eval_set = [(X_val_concatenated, y_val)], eval_metric = \"logloss\")\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7bc7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0640b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('models_lgb.pkl', 'wb') as handle:\n",
    "#    pickle.dump(lgb_model, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ac5774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 69639, number of negative: 69604\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 139243, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1240]\tvalid_0's binary_logloss: 0.202364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'early_stopping_rounds': 50,\n",
       " 'learning_rate': 0.01,\n",
       " 'min_child_samples': 100,\n",
       " 'n_estimators': 2000,\n",
       " 'num_leaves': 50,\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': 0.3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model_cnn = lgb.LGBMClassifier()\n",
    "grid_search_cnn = GridSearchCV(estimator = lgb_model_cnn, param_grid = param_grid, cv = 5,\n",
    "                          n_jobs = -1, verbose = 2, scoring = \"f1\") \n",
    "grid_search_cnn.fit(X_train_cnn_concatenated, y_train, eval_set = [(X_val_cnn_concatenated, y_val)], eval_metric = \"logloss\")\n",
    "grid_search_cnn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0d5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model_cnn = grid_search_cnn.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5de5f87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.826477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.202596\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.1; total time= 8.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.599273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1998]\tvalid_0's binary_logloss: 0.202597\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.2; total time= 7.9min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.547466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.202574\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.3; total time= 8.0min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.757977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1865]\tvalid_0's binary_logloss: 0.201821\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.1; total time=11.6min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.510645 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1370]\tvalid_0's binary_logloss: 0.205897\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.1; total time= 8.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.513223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1725]\tvalid_0's binary_logloss: 0.202326\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.2; total time= 9.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.554316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1367]\tvalid_0's binary_logloss: 0.202924\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.3; total time= 7.7min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.532251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1075]\tvalid_0's binary_logloss: 0.202302\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.1; total time= 8.8min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.559963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1009]\tvalid_0's binary_logloss: 0.205894\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.1; total time= 7.6min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.480410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1055]\tvalid_0's binary_logloss: 0.202739\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.2; total time= 8.0min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.481379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1343]\tvalid_0's binary_logloss: 0.202281\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.3; total time=10.0min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.481603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[955]\tvalid_0's binary_logloss: 0.202021\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.1; total time= 8.9min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.551630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[638]\tvalid_0's binary_logloss: 0.205794\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.1; total time= 6.0min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.496397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[912]\tvalid_0's binary_logloss: 0.20281\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.2; total time= 8.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.498978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1049]\tvalid_0's binary_logloss: 0.202504\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.3; total time= 9.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.851098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.202411\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.1; total time= 8.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.553984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.202961\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.2; total time= 7.8min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.619009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.202415\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.3; total time= 8.0min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.563688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1994]\tvalid_0's binary_logloss: 0.205877\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.3; total time= 7.7min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.506274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1444]\tvalid_0's binary_logloss: 0.202518\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.1; total time= 8.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.529531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1689]\tvalid_0's binary_logloss: 0.201988\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.2; total time= 9.9min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.510490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1869]\tvalid_0's binary_logloss: 0.201779\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.3; total time=10.1min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.542052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1626]\tvalid_0's binary_logloss: 0.205686\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.3; total time= 8.9min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.523305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1299]\tvalid_0's binary_logloss: 0.202282\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.1; total time= 9.6min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.466045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1282]\tvalid_0's binary_logloss: 0.202137\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.2; total time= 9.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.526638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1091]\tvalid_0's binary_logloss: 0.202699\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.3; total time= 8.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.512568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[985]\tvalid_0's binary_logloss: 0.202473\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.1; total time= 9.1min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.484691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[913]\tvalid_0's binary_logloss: 0.202189\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.2; total time= 8.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.489910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.205915\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.2; total time= 5.8min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.572795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[943]\tvalid_0's binary_logloss: 0.20291\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.3; total time= 8.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.777558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.202942\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.1; total time= 8.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.723674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.20236\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.2; total time= 7.8min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.621612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.206019\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.2; total time= 7.8min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.779098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1993]\tvalid_0's binary_logloss: 0.202725\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.3; total time= 7.9min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.637501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1687]\tvalid_0's binary_logloss: 0.202339\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.1; total time= 9.6min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.502406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.202036\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.2; total time=11.1min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.524918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1856]\tvalid_0's binary_logloss: 0.202055\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.3; total time=10.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.491537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1272]\tvalid_0's binary_logloss: 0.20235\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.1; total time= 9.8min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.483443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1087]\tvalid_0's binary_logloss: 0.202136\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.2; total time= 8.2min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.507452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1036]\tvalid_0's binary_logloss: 0.205805\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.2; total time= 7.8min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.490319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1274]\tvalid_0's binary_logloss: 0.202108\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.3; total time= 9.5min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.491201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[982]\tvalid_0's binary_logloss: 0.20264\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.1; total time= 9.0min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.478641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1050]\tvalid_0's binary_logloss: 0.202336\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.2; total time= 9.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.488369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1017]\tvalid_0's binary_logloss: 0.202062\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.3; total time= 9.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.489929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[632]\tvalid_0's binary_logloss: 0.205717\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.3; total time= 4.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.829839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1979]\tvalid_0's binary_logloss: 0.203056\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.1; total time= 8.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.675606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1996]\tvalid_0's binary_logloss: 0.205926\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.1; total time= 7.6min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.925352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.202848\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.2; total time= 8.0min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.787032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1994]\tvalid_0's binary_logloss: 0.203014\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=15, random_state=42, reg_alpha=0.3; total time= 7.9min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.628472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1744]\tvalid_0's binary_logloss: 0.202794\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.1; total time= 9.9min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.525879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1351]\tvalid_0's binary_logloss: 0.202927\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.2; total time= 8.1min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.494038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1370]\tvalid_0's binary_logloss: 0.205921\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.2; total time= 7.7min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.492814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1646]\tvalid_0's binary_logloss: 0.202246\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=30, random_state=42, reg_alpha=0.3; total time= 8.9min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.455337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1105]\tvalid_0's binary_logloss: 0.202679\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.1; total time= 9.1min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.553538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1268]\tvalid_0's binary_logloss: 0.202328\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.2; total time= 9.4min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.486083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111394, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1255]\tvalid_0's binary_logloss: 0.202124\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.3; total time= 9.5min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55711, number of negative: 55684\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.499213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500121 -> initscore=0.000485\n",
      "[LightGBM] [Info] Start training from score 0.000485\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1064]\tvalid_0's binary_logloss: 0.205637\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=50, random_state=42, reg_alpha=0.3; total time= 7.8min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.533035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1028]\tvalid_0's binary_logloss: 0.202413\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.1; total time= 9.3min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.460148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1126]\tvalid_0's binary_logloss: 0.202126\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.2; total time=10.1min\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 55712, number of negative: 55683\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.471699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 111395, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500130 -> initscore=0.000521\n",
      "[LightGBM] [Info] Start training from score 0.000521\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1159]\tvalid_0's binary_logloss: 0.202137\n",
      "[CV] END early_stopping_rounds=50, learning_rate=0.01, min_child_samples=100, n_estimators=2000, num_leaves=70, random_state=42, reg_alpha=0.3; total time= 8.5min\n"
     ]
    }
   ],
   "source": [
    "# with open('models_lgb_cnn.pkl', 'wb') as handle:\n",
    "#     pickle.dump(lgb_model_cnn, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db22af0",
   "metadata": {},
   "source": [
    "# Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ee999604",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v_lr = tf.keras.models.load_model(\"model_d2v_lr.keras\")\n",
    "model_d2v_nn = tf.keras.models.load_model(\"model_d2v_nn.keras\")\n",
    "model_we = tf.keras.models.load_model(\"model_word_embedding.keras\")\n",
    "model_cnn = tf.keras.models.load_model(\"model_cnn_1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aa0c292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_lgb.pkl', 'rb') as handle:\n",
    "    model_lgb_we = pickle.load(handle)\n",
    "with open('models_lgb_cnn.pkl', 'rb') as handle:\n",
    "    model_lgb_cnn = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a1bf8b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1451/1451 [==============================] - 3s 2ms/step\n",
      "1451/1451 [==============================] - 11s 7ms/step\n",
      "1451/1451 [==============================] - 2s 1ms/step\n",
      "1451/1451 [==============================] - 2s 1ms/step\n",
      "1451/1451 [==============================] - 3s 2ms/step\n",
      "1451/1451 [==============================] - 11s 8ms/step\n",
      "1451/1451 [==============================] - 2s 1ms/step\n",
      "1451/1451 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "we_preds = model_we.predict(X_val_reduced).flatten()\n",
    "cnn_preds = model_cnn.predict(X_val_reduced).flatten()\n",
    "sc_x = StandardScaler()\n",
    "d2v_lr_preds = model_d2v_lr.predict(sc_x.fit(X_train_sent_embedded).transform(X_val_sent_embedded)).flatten()\n",
    "d2v_nn_preds = model_d2v_nn.predict(X_val_sent_embedded).flatten()\n",
    "lgb_we_preds = model_lgb_we.predict_proba(X_val_concatenated)[:, 1]\n",
    "lgb_cnn_preds = model_lgb_cnn.predict_proba(X_val_cnn_concatenated)[:, 1]\n",
    "\n",
    "we_preds_test = model_we.predict(X_test_reduced).flatten()\n",
    "cnn_preds_test = model_cnn.predict(X_test_reduced).flatten()\n",
    "sc_x = StandardScaler()\n",
    "d2v_lr_preds_test = model_d2v_lr.predict(sc_x.fit(X_train_sent_embedded).transform(X_test_sent_embedded)).flatten()\n",
    "d2v_nn_preds_test = model_d2v_nn.predict(X_test_sent_embedded).flatten()\n",
    "lgb_we_preds_test = model_lgb_we.predict_proba(X_test_concatenated)[:, 1]\n",
    "lgb_cnn_preds_test = model_lgb_cnn.predict_proba(X_test_cnn_concatenated)[:, 1]\n",
    "\n",
    "\n",
    "we_preds_int = np.round(we_preds).astype(int)\n",
    "cnn_preds_int = np.round(cnn_preds).astype(int)\n",
    "d2v_lr_preds_int = np.round(d2v_lr_preds).astype(int)\n",
    "d2v_nn_preds_int = np.round(d2v_nn_preds).astype(int)\n",
    "lgb_we_preds_int = np.round(lgb_we_preds).astype(int)\n",
    "lgb_cnn_preds_int = np.round(lgb_we_preds).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3fe9781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_val_preds = pd.DataFrame({\"Word Embedding Preds\": we_preds,\n",
    "                                 \"CNN Embedding Preds\": cnn_preds,\n",
    "                                 \"Doc2Vec LR Preds\": d2v_lr_preds,\n",
    "                                 \"Doc2Vec NN Preds\": d2v_nn_preds,   \n",
    "                                 \"Word Embedding LGB Preds\": lgb_we_preds,\n",
    "                                 \"CNN Embedding LGB Preds\": lgb_cnn_preds})\n",
    "embedding_test_preds = pd.DataFrame({\"Word Embedding Preds\": we_preds_test,\n",
    "                                 \"CNN Embedding Preds\": cnn_preds_test,\n",
    "                                 \"Doc2Vec LR Preds\": d2v_lr_preds_test,\n",
    "                                 \"Doc2Vec NN Preds\": d2v_nn_preds_test,\n",
    "                                 \"Word Embedding LGB Preds\": lgb_we_preds_test,\n",
    "                                 \"CNN Embedding LGB Preds\": lgb_cnn_preds_test})\n",
    "with open(\"embedding_val_preds.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(embedding_val_preds, handle)\n",
    "with open(\"embedding_test_preds.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(embedding_test_preds, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6e98a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     23185\n",
      "           1       0.92      0.92      0.92     23230\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, we_preds_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9f00e54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92     23185\n",
      "           1       0.91      0.94      0.92     23230\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, cnn_preds_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "532fa2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86     23185\n",
      "           1       0.89      0.80      0.84     23230\n",
      "\n",
      "    accuracy                           0.85     46415\n",
      "   macro avg       0.85      0.85      0.85     46415\n",
      "weighted avg       0.85      0.85      0.85     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, d2v_lr_preds_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1b495e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88     23185\n",
      "           1       0.89      0.87      0.88     23230\n",
      "\n",
      "    accuracy                           0.88     46415\n",
      "   macro avg       0.88      0.88      0.88     46415\n",
      "weighted avg       0.88      0.88      0.88     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, d2v_nn_preds_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "35abd873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of d2v predictions correct over other models:\n",
      " 1.57\n",
      "Class of d2v predictions that are correct over other models:\n",
      " Actual\n",
      "0    0.682192\n",
      "1    0.317808\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "val_results = pd.DataFrame({\"Actual\": y_val,\n",
    "                           \"Word Embedding\": we_preds_int,\n",
    "                           \"1D CNN\": cnn_preds_int, \n",
    "                           \"Sentence Embedding\": d2v_nn_preds_int})\n",
    "d2v_correct_over_others = val_results[(val_results[\"Sentence Embedding\"] != val_results[\"Word Embedding\"]) & \n",
    "           (val_results[\"Sentence Embedding\"] != val_results[\"1D CNN\"]) & \n",
    "           (val_results[\"Sentence Embedding\"] == val_results[\"Actual\"])]\n",
    "print(\"% of d2v predictions correct over other models:\\n\", round(d2v_correct_over_others.shape[0]/val_results.shape[0] * 100, 2))\n",
    "print(\"Class of d2v predictions that are correct over other models:\\n\", d2v_correct_over_others[\"Actual\"].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81c47c",
   "metadata": {},
   "source": [
    "The original word embeddings model is very consistent across all the metrics, scoring around 92% across all metrics on the validation set. The embeddings created with 1D convolutional neural network have the same F1 score as the original word embeddings, however, there is slight variations in precision and recall. The 1D CNN has slightly higher precision / lower recall for non-suicide and the reverse for the suicide class. The 1D CNN model is slightly more conservative with predicting non-suicide class, but in doing so is more accurate in predicting them. The Doc2Vec embedding neural network model has a lower score on all metrics. It tends to be more conservative predicting a class as suicide (indicated with a lower recall score). In doing so, of the entries that the Doc2Vec neural network predicts correctly over the word embedding models, 2/3 of them are from the non-suicide class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3643c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     23185\n",
      "           1       0.92      0.92      0.92     23230\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, lgb_we_preds_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2815e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     23185\n",
      "           1       0.92      0.92      0.92     23230\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, lgb_cnn_preds_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bde8e3b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB Preds same: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"LGB Preds same:\", sum(lgb_cnn_preds_int == lgb_we_preds_int)/lgb_we_preds_int.shape[0])\n",
    "pd.Series(abs(lgb_we_preds - lgb_cnn_preds)).hist(label = \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a5a64187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff < 10%: 0.8731444576106862\n",
      "Diff 10-20%: 0.0650867176559302\n",
      "Diff 20-30%: 0.030248841969190993\n",
      "Diff 30+%: 0.03151998276419261\n"
     ]
    }
   ],
   "source": [
    "print(\"Diff < 10%:\", sum(abs(lgb_we_preds - lgb_cnn_preds) < 0.1)/lgb_we_preds.shape[0])\n",
    "print(\"Diff 10-20%:\", sum((abs(lgb_we_preds - lgb_cnn_preds) >= 0.1) & \n",
    "                         (abs(lgb_we_preds - lgb_cnn_preds) < 0.2))/lgb_we_preds.shape[0])\n",
    "print(\"Diff 20-30%:\", sum((abs(lgb_we_preds - lgb_cnn_preds) >= 0.2) & \n",
    "                         (abs(lgb_we_preds - lgb_cnn_preds) < 0.3))/lgb_we_preds.shape[0])\n",
    "\n",
    "print(\"Diff 30+%:\", sum((abs(lgb_we_preds - lgb_cnn_preds) >= 0.3))/lgb_we_preds.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e4aae",
   "metadata": {},
   "source": [
    "Both the boosting model yield the same predictions, and looking at the prediction probabilities, 87% of the predicted probabilities are within 10% of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf67af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
