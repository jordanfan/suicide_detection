{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9005b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "import plotly.graph_objs as plotly  # for interactive plots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70656f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_embeddings_tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "model_we = tf.keras.models.load_model(\"model_word_embedding.keras\")\n",
    "model_cnn = tf.keras.models.load_model(\"model_cnn_1.keras\")\n",
    "\n",
    "with open('X_train_reduced.pkl', 'rb') as handle:\n",
    "    X_train_reduced = pickle.load(handle)\n",
    "with open('X_val_reduced.pkl', 'rb') as handle:\n",
    "    X_val_reduced = pickle.load(handle)\n",
    "with open('X_test_reduced.pkl', 'rb') as handle:\n",
    "    X_test_reduced = pickle.load(handle)\n",
    "    \n",
    "with open(\"y_train.pkl\", 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "with open(\"y_val.pkl\", 'rb') as handle:\n",
    "    y_val = pickle.load(handle)\n",
    "with open(\"y_test.pkl\", 'rb') as handle:\n",
    "    y_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81f43f",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a7dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lr_model(X_train, y_train, learning_rate=0.01, random_state = 1234, epochs = 100):\n",
    "    \"\"\"Build a TF logistic regression model using Keras.\n",
    "\n",
    "    Args:\n",
    "    learning_rate: The desired learning rate for Adam.\n",
    "\n",
    "    Returns:\n",
    "    model: A tf.keras model (graph).\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    np.random.seed(random_state)\n",
    "    tf.random.set_seed(random_state)\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) \n",
    "    \n",
    "    # Build a model using keras.Sequential.\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Keras layers can do pre-processing. This layer will take 100x100 embeddings\n",
    "    # and flatten them into vectors of size 10000.\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # This layer constructs the linear set of parameters for each input feature\n",
    "    # (as well as a bias), and applies a sigmoid to the result. The result is\n",
    "    # binary logistic regression.\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "      units=1,                     # output dim (for binary classification)\n",
    "      use_bias=True,               # use a bias param\n",
    "      activation=\"sigmoid\"         # apply the sigmoid function!\n",
    "    ))\n",
    "\n",
    "    # Use the Adam optimizer.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                optimizer=optimizer, \n",
    "                metrics=[\"accuracy\"])\n",
    "    history = model.fit(\n",
    "        x = X_train,\n",
    "        y = y_train,\n",
    "        validation_split=0.2, \n",
    "        epochs=epochs, \n",
    "        callbacks=[callback])\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7600194",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b58eb5fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Learning Rate: 0.0001\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.6306 - accuracy: 0.6022 - val_loss: 0.5880 - val_accuracy: 0.7084\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.5552 - accuracy: 0.7581 - val_loss: 0.5314 - val_accuracy: 0.7934\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.5087 - accuracy: 0.8093 - val_loss: 0.4945 - val_accuracy: 0.8170\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.4774 - accuracy: 0.8256 - val_loss: 0.4689 - val_accuracy: 0.8270\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.4552 - accuracy: 0.8333 - val_loss: 0.4502 - val_accuracy: 0.8334\n",
      "Epoch 6/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.4387 - accuracy: 0.8377 - val_loss: 0.4360 - val_accuracy: 0.8359\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4260 - accuracy: 0.8402 - val_loss: 0.4249 - val_accuracy: 0.8378\n",
      "Epoch 8/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4160 - accuracy: 0.8427 - val_loss: 0.4160 - val_accuracy: 0.8402\n",
      "Epoch 9/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4079 - accuracy: 0.8444 - val_loss: 0.4089 - val_accuracy: 0.8419\n",
      "Epoch 10/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4012 - accuracy: 0.8457 - val_loss: 0.4030 - val_accuracy: 0.8430\n",
      "Epoch 11/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3957 - accuracy: 0.8466 - val_loss: 0.3979 - val_accuracy: 0.8445\n",
      "Epoch 12/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3911 - accuracy: 0.8475 - val_loss: 0.3936 - val_accuracy: 0.8450\n",
      "Epoch 13/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3872 - accuracy: 0.8481 - val_loss: 0.3900 - val_accuracy: 0.8458\n",
      "Epoch 14/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3838 - accuracy: 0.8485 - val_loss: 0.3869 - val_accuracy: 0.8462\n",
      "Epoch 15/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3809 - accuracy: 0.8493 - val_loss: 0.3842 - val_accuracy: 0.8469\n",
      "Epoch 16/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3783 - accuracy: 0.8500 - val_loss: 0.3819 - val_accuracy: 0.8476\n",
      "Epoch 17/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3761 - accuracy: 0.8503 - val_loss: 0.3797 - val_accuracy: 0.8478\n",
      "Epoch 18/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3741 - accuracy: 0.8509 - val_loss: 0.3779 - val_accuracy: 0.8489\n",
      "Epoch 19/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3723 - accuracy: 0.8513 - val_loss: 0.3762 - val_accuracy: 0.8493\n",
      "Epoch 20/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3707 - accuracy: 0.8516 - val_loss: 0.3747 - val_accuracy: 0.8497\n",
      "Epoch 21/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3693 - accuracy: 0.8522 - val_loss: 0.3734 - val_accuracy: 0.8497\n",
      "Epoch 22/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3680 - accuracy: 0.8522 - val_loss: 0.3722 - val_accuracy: 0.8502\n",
      "Epoch 23/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3668 - accuracy: 0.8526 - val_loss: 0.3711 - val_accuracy: 0.8504\n",
      "Epoch 24/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3658 - accuracy: 0.8530 - val_loss: 0.3702 - val_accuracy: 0.8509\n",
      "Epoch 25/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3648 - accuracy: 0.8531 - val_loss: 0.3692 - val_accuracy: 0.8510\n",
      "Epoch 26/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3640 - accuracy: 0.8532 - val_loss: 0.3684 - val_accuracy: 0.8516\n",
      "Epoch 27/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3632 - accuracy: 0.8536 - val_loss: 0.3676 - val_accuracy: 0.8519\n",
      "Epoch 28/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3624 - accuracy: 0.8535 - val_loss: 0.3669 - val_accuracy: 0.8523\n",
      "Epoch 29/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3617 - accuracy: 0.8538 - val_loss: 0.3664 - val_accuracy: 0.8522\n",
      "Epoch 30/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3611 - accuracy: 0.8540 - val_loss: 0.3657 - val_accuracy: 0.8531\n",
      "Epoch 31/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3605 - accuracy: 0.8543 - val_loss: 0.3652 - val_accuracy: 0.8528\n",
      "Epoch 32/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3600 - accuracy: 0.8547 - val_loss: 0.3647 - val_accuracy: 0.8532\n",
      "Epoch 33/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3595 - accuracy: 0.8548 - val_loss: 0.3642 - val_accuracy: 0.8536\n",
      "Epoch 34/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3590 - accuracy: 0.8551 - val_loss: 0.3638 - val_accuracy: 0.8536\n",
      "Epoch 35/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3586 - accuracy: 0.8553 - val_loss: 0.3634 - val_accuracy: 0.8534\n",
      "Epoch 36/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3582 - accuracy: 0.8552 - val_loss: 0.3630 - val_accuracy: 0.8538\n",
      "Epoch 37/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3578 - accuracy: 0.8551 - val_loss: 0.3626 - val_accuracy: 0.8540\n",
      "Epoch 38/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3574 - accuracy: 0.8554 - val_loss: 0.3623 - val_accuracy: 0.8541\n",
      "Epoch 39/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3571 - accuracy: 0.8557 - val_loss: 0.3621 - val_accuracy: 0.8541\n",
      "Epoch 40/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3568 - accuracy: 0.8559 - val_loss: 0.3618 - val_accuracy: 0.8544\n",
      "Epoch 41/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3565 - accuracy: 0.8556 - val_loss: 0.3614 - val_accuracy: 0.8541\n",
      "Epoch 42/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3562 - accuracy: 0.8562 - val_loss: 0.3612 - val_accuracy: 0.8542\n",
      "Epoch 43/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3560 - accuracy: 0.8561 - val_loss: 0.3610 - val_accuracy: 0.8546\n",
      "Epoch 44/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3557 - accuracy: 0.8562 - val_loss: 0.3608 - val_accuracy: 0.8548\n",
      "Epoch 45/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3555 - accuracy: 0.8562 - val_loss: 0.3606 - val_accuracy: 0.8548\n",
      "Epoch 46/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3553 - accuracy: 0.8566 - val_loss: 0.3604 - val_accuracy: 0.8547\n",
      "Epoch 47/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3551 - accuracy: 0.8564 - val_loss: 0.3601 - val_accuracy: 0.8548\n",
      "Epoch 48/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3549 - accuracy: 0.8564 - val_loss: 0.3602 - val_accuracy: 0.8543\n",
      "Epoch 49/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3547 - accuracy: 0.8569 - val_loss: 0.3598 - val_accuracy: 0.8548\n",
      "Epoch 50/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3545 - accuracy: 0.8567 - val_loss: 0.3597 - val_accuracy: 0.8549\n",
      "Epoch 51/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3544 - accuracy: 0.8567 - val_loss: 0.3595 - val_accuracy: 0.8550\n",
      "Epoch 52/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3542 - accuracy: 0.8568 - val_loss: 0.3594 - val_accuracy: 0.8550\n",
      "Epoch 53/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3541 - accuracy: 0.8567 - val_loss: 0.3593 - val_accuracy: 0.8550\n",
      "Epoch 54/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3539 - accuracy: 0.8568 - val_loss: 0.3591 - val_accuracy: 0.8550\n",
      "Epoch 55/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3538 - accuracy: 0.8568 - val_loss: 0.3591 - val_accuracy: 0.8549\n",
      "Epoch 56/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3537 - accuracy: 0.8570 - val_loss: 0.3589 - val_accuracy: 0.8553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3536 - accuracy: 0.8570 - val_loss: 0.3588 - val_accuracy: 0.8553\n",
      "Epoch 58/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3535 - accuracy: 0.8571 - val_loss: 0.3587 - val_accuracy: 0.8551\n",
      "Epoch 59/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3534 - accuracy: 0.8572 - val_loss: 0.3587 - val_accuracy: 0.8552\n",
      "Epoch 60/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3533 - accuracy: 0.8570 - val_loss: 0.3587 - val_accuracy: 0.8554\n",
      "Epoch 61/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3532 - accuracy: 0.8572 - val_loss: 0.3585 - val_accuracy: 0.8553\n",
      "Epoch 62/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3531 - accuracy: 0.8573 - val_loss: 0.3584 - val_accuracy: 0.8555\n",
      "Epoch 63/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3530 - accuracy: 0.8575 - val_loss: 0.3583 - val_accuracy: 0.8553\n",
      "Epoch 64/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3529 - accuracy: 0.8573 - val_loss: 0.3583 - val_accuracy: 0.8558\n",
      "Epoch 65/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3528 - accuracy: 0.8575 - val_loss: 0.3582 - val_accuracy: 0.8553\n",
      "Epoch 66/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3528 - accuracy: 0.8577 - val_loss: 0.3582 - val_accuracy: 0.8558\n",
      "Epoch 67/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3527 - accuracy: 0.8576 - val_loss: 0.3580 - val_accuracy: 0.8551\n",
      "Epoch 68/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3526 - accuracy: 0.8574 - val_loss: 0.3580 - val_accuracy: 0.8557\n",
      "Epoch 69/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3525 - accuracy: 0.8576 - val_loss: 0.3580 - val_accuracy: 0.8557\n",
      "Epoch 70/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3525 - accuracy: 0.8574 - val_loss: 0.3579 - val_accuracy: 0.8556\n",
      "Epoch 71/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3524 - accuracy: 0.8576 - val_loss: 0.3578 - val_accuracy: 0.8555\n",
      "Epoch 72/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3524 - accuracy: 0.8576 - val_loss: 0.3578 - val_accuracy: 0.8556\n",
      "Epoch 73/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3523 - accuracy: 0.8575 - val_loss: 0.3579 - val_accuracy: 0.8559\n",
      "Epoch 74/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3523 - accuracy: 0.8576 - val_loss: 0.3578 - val_accuracy: 0.8557\n",
      "Epoch 75/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3522 - accuracy: 0.8576 - val_loss: 0.3577 - val_accuracy: 0.8560\n",
      "Epoch 76/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3522 - accuracy: 0.8575 - val_loss: 0.3577 - val_accuracy: 0.8557\n",
      "Epoch 77/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3521 - accuracy: 0.8576 - val_loss: 0.3576 - val_accuracy: 0.8559\n",
      "Epoch 78/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3521 - accuracy: 0.8577 - val_loss: 0.3576 - val_accuracy: 0.8558\n",
      "Epoch 79/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3521 - accuracy: 0.8578 - val_loss: 0.3575 - val_accuracy: 0.8559\n",
      "Epoch 80/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3520 - accuracy: 0.8579 - val_loss: 0.3575 - val_accuracy: 0.8559\n",
      "Epoch 81/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3520 - accuracy: 0.8578 - val_loss: 0.3575 - val_accuracy: 0.8559\n",
      "Epoch 82/100\n",
      "3482/3482 [==============================] - 6s 2ms/step - loss: 0.3520 - accuracy: 0.8578 - val_loss: 0.3575 - val_accuracy: 0.8562\n",
      "Epoch 83/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3519 - accuracy: 0.8579 - val_loss: 0.3575 - val_accuracy: 0.8560\n",
      "Epoch 84/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3519 - accuracy: 0.8579 - val_loss: 0.3575 - val_accuracy: 0.8561\n",
      "Epoch 85/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3519 - accuracy: 0.8579 - val_loss: 0.3574 - val_accuracy: 0.8561\n",
      "Epoch 86/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3518 - accuracy: 0.8579 - val_loss: 0.3574 - val_accuracy: 0.8561\n",
      "Epoch 87/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3518 - accuracy: 0.8579 - val_loss: 0.3573 - val_accuracy: 0.8559\n",
      "Epoch 88/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3518 - accuracy: 0.8578 - val_loss: 0.3573 - val_accuracy: 0.8553\n",
      "Epoch 89/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3518 - accuracy: 0.8579 - val_loss: 0.3573 - val_accuracy: 0.8562\n",
      "Epoch 90/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3517 - accuracy: 0.8579 - val_loss: 0.3574 - val_accuracy: 0.8564\n",
      "Epoch 91/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3517 - accuracy: 0.8581 - val_loss: 0.3572 - val_accuracy: 0.8555\n",
      "Epoch 92/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3517 - accuracy: 0.8579 - val_loss: 0.3572 - val_accuracy: 0.8557\n",
      "Epoch 93/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3516 - accuracy: 0.8581 - val_loss: 0.3572 - val_accuracy: 0.8555\n",
      "Epoch 94/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3516 - accuracy: 0.8580 - val_loss: 0.3572 - val_accuracy: 0.8557\n",
      "Epoch 95/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3516 - accuracy: 0.8581 - val_loss: 0.3573 - val_accuracy: 0.8564\n",
      "Epoch 96/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3516 - accuracy: 0.8580 - val_loss: 0.3572 - val_accuracy: 0.8561\n",
      "Epoch 97/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3516 - accuracy: 0.8581 - val_loss: 0.3571 - val_accuracy: 0.8557\n",
      "Epoch 98/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3516 - accuracy: 0.8581 - val_loss: 0.3572 - val_accuracy: 0.8561\n",
      "Epoch 99/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3515 - accuracy: 0.8580 - val_loss: 0.3571 - val_accuracy: 0.8557\n",
      "Epoch 100/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3515 - accuracy: 0.8582 - val_loss: 0.3572 - val_accuracy: 0.8563\n",
      "\n",
      "Training Learning Rate: 0.001\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.4791 - accuracy: 0.7993 - val_loss: 0.4093 - val_accuracy: 0.8415\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3852 - accuracy: 0.8480 - val_loss: 0.3769 - val_accuracy: 0.8483\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3666 - accuracy: 0.8523 - val_loss: 0.3668 - val_accuracy: 0.8521\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3596 - accuracy: 0.8544 - val_loss: 0.3621 - val_accuracy: 0.8531\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3562 - accuracy: 0.8557 - val_loss: 0.3600 - val_accuracy: 0.8544\n",
      "Epoch 6/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3545 - accuracy: 0.8567 - val_loss: 0.3587 - val_accuracy: 0.8549\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3535 - accuracy: 0.8571 - val_loss: 0.3580 - val_accuracy: 0.8549\n",
      "Epoch 8/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3528 - accuracy: 0.8576 - val_loss: 0.3574 - val_accuracy: 0.8559\n",
      "Epoch 9/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3524 - accuracy: 0.8572 - val_loss: 0.3574 - val_accuracy: 0.8560\n",
      "Epoch 10/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3522 - accuracy: 0.8574 - val_loss: 0.3578 - val_accuracy: 0.8569\n",
      "Epoch 11/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3520 - accuracy: 0.8576 - val_loss: 0.3571 - val_accuracy: 0.8560\n",
      "Epoch 12/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3519 - accuracy: 0.8577 - val_loss: 0.3572 - val_accuracy: 0.8557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3519 - accuracy: 0.8580 - val_loss: 0.3572 - val_accuracy: 0.8562\n",
      "Epoch 14/100\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.3518 - accuracy: 0.8579 - val_loss: 0.3570 - val_accuracy: 0.8562\n",
      "Epoch 15/100\n",
      "3482/3482 [==============================] - 9s 3ms/step - loss: 0.3518 - accuracy: 0.8581 - val_loss: 0.3571 - val_accuracy: 0.8560\n",
      "Epoch 16/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3519 - accuracy: 0.8582 - val_loss: 0.3572 - val_accuracy: 0.8564\n",
      "Epoch 17/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3518 - accuracy: 0.8578 - val_loss: 0.3574 - val_accuracy: 0.8553\n",
      "Epoch 18/100\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.3518 - accuracy: 0.8582 - val_loss: 0.3570 - val_accuracy: 0.8563\n",
      "Epoch 19/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3518 - accuracy: 0.8584 - val_loss: 0.3570 - val_accuracy: 0.8562\n",
      "Epoch 20/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3517 - accuracy: 0.8581 - val_loss: 0.3571 - val_accuracy: 0.8568\n",
      "Epoch 21/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3518 - accuracy: 0.8584 - val_loss: 0.3570 - val_accuracy: 0.8565\n",
      "Epoch 22/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3518 - accuracy: 0.8583 - val_loss: 0.3572 - val_accuracy: 0.8567\n",
      "Epoch 23/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3518 - accuracy: 0.8581 - val_loss: 0.3580 - val_accuracy: 0.8569\n",
      "\n",
      "Training Learning Rate: 0.01\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3762 - accuracy: 0.8458 - val_loss: 0.3606 - val_accuracy: 0.8524\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3566 - accuracy: 0.8553 - val_loss: 0.3592 - val_accuracy: 0.8549\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3564 - accuracy: 0.8560 - val_loss: 0.3598 - val_accuracy: 0.8542\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3564 - accuracy: 0.8557 - val_loss: 0.3614 - val_accuracy: 0.8520\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3560 - accuracy: 0.8567 - val_loss: 0.3602 - val_accuracy: 0.8558\n",
      "Epoch 6/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3564 - accuracy: 0.8565 - val_loss: 0.3585 - val_accuracy: 0.8563\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 9s 2ms/step - loss: 0.3563 - accuracy: 0.8555 - val_loss: 0.3628 - val_accuracy: 0.8537\n",
      "Epoch 8/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3563 - accuracy: 0.8555 - val_loss: 0.3588 - val_accuracy: 0.8554\n",
      "Epoch 9/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3561 - accuracy: 0.8557 - val_loss: 0.3616 - val_accuracy: 0.8551\n",
      "Epoch 10/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3560 - accuracy: 0.8562 - val_loss: 0.3604 - val_accuracy: 0.8547\n",
      "Epoch 11/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3563 - accuracy: 0.8554 - val_loss: 0.3587 - val_accuracy: 0.8551\n",
      "\n",
      "Training Learning Rate: 0.1\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 0.3981 - accuracy: 0.8397 - val_loss: 0.3979 - val_accuracy: 0.8390\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4013 - accuracy: 0.8385 - val_loss: 0.4240 - val_accuracy: 0.8340\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4021 - accuracy: 0.8382 - val_loss: 0.3997 - val_accuracy: 0.8429\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4019 - accuracy: 0.8382 - val_loss: 0.4224 - val_accuracy: 0.8329\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4009 - accuracy: 0.8389 - val_loss: 0.3853 - val_accuracy: 0.8465\n",
      "Epoch 6/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4015 - accuracy: 0.8398 - val_loss: 0.3882 - val_accuracy: 0.8520\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3992 - accuracy: 0.8401 - val_loss: 0.3946 - val_accuracy: 0.8437\n",
      "Epoch 8/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4020 - accuracy: 0.8390 - val_loss: 0.4237 - val_accuracy: 0.8240\n",
      "Epoch 9/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.4004 - accuracy: 0.8396 - val_loss: 0.3983 - val_accuracy: 0.8393\n",
      "Epoch 10/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 0.3986 - accuracy: 0.8402 - val_loss: 0.3991 - val_accuracy: 0.8433\n",
      "\n",
      "Training Learning Rate: 1\n",
      "Epoch 1/100\n",
      "3482/3482 [==============================] - 10s 3ms/step - loss: 1.0211 - accuracy: 0.7994 - val_loss: 1.6938 - val_accuracy: 0.7284\n",
      "Epoch 2/100\n",
      "3482/3482 [==============================] - 8s 2ms/step - loss: 1.0698 - accuracy: 0.8015 - val_loss: 1.5650 - val_accuracy: 0.7716\n",
      "Epoch 3/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.0938 - accuracy: 0.8012 - val_loss: 1.0133 - val_accuracy: 0.8102\n",
      "Epoch 4/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.0461 - accuracy: 0.8013 - val_loss: 1.0208 - val_accuracy: 0.8062\n",
      "Epoch 5/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.0956 - accuracy: 0.8003 - val_loss: 1.1766 - val_accuracy: 0.7740\n",
      "Epoch 6/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.0633 - accuracy: 0.8026 - val_loss: 1.4190 - val_accuracy: 0.7308\n",
      "Epoch 7/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.1288 - accuracy: 0.7994 - val_loss: 0.8820 - val_accuracy: 0.8190\n",
      "Epoch 8/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.0812 - accuracy: 0.7994 - val_loss: 1.8294 - val_accuracy: 0.7156\n",
      "Epoch 9/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.0835 - accuracy: 0.8022 - val_loss: 0.9257 - val_accuracy: 0.8108\n",
      "Epoch 10/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.0461 - accuracy: 0.8018 - val_loss: 0.9658 - val_accuracy: 0.8026\n",
      "Epoch 11/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.0803 - accuracy: 0.8019 - val_loss: 0.8915 - val_accuracy: 0.8064\n",
      "Epoch 12/100\n",
      "3482/3482 [==============================] - 7s 2ms/step - loss: 1.1076 - accuracy: 0.7997 - val_loss: 0.9965 - val_accuracy: 0.8094\n"
     ]
    }
   ],
   "source": [
    "models_d2v = []\n",
    "histories_d2v = []\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for lr in learning_rates: \n",
    "    print(f\"\\nTraining Learning Rate: {lr}\")\n",
    "    model, history = build_lr_model(X_train_sent_embedded, y_train, learning_rate = lr)\n",
    "    models_d2v.append(model)\n",
    "    histories_d2v.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('models_d2v.pkl', 'wb') as handle:\n",
    "#     pickle.dump(models_d2v, handle)\n",
    "# with open('histories_d2v.pkl', 'wb') as handle:\n",
    "#     pickle.dump(histories_d2v, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8e02c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v = models_d2v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5cab9ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1451/1451 [==============================] - 3s 2ms/step\n",
      "1451/1451 [==============================] - 12s 8ms/step\n",
      "1451/1451 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "we_preds = np.round(model_we.predict(X_val_reduced)).flatten().astype(int)\n",
    "cnn_preds = np.round(model_cnn.predict(X_val_reduced)).flatten().astype(int)\n",
    "d2v_preds = np.round(model_d2v.predict(X_val_sent_embedded)).flatten().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2213c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92     23185\n",
      "           1       0.93      0.92      0.92     23230\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, we_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6d05ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     23185\n",
      "           1       0.92      0.92      0.92     23230\n",
      "\n",
      "    accuracy                           0.92     46415\n",
      "   macro avg       0.92      0.92      0.92     46415\n",
      "weighted avg       0.92      0.92      0.92     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, cnn_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a5caeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86     23185\n",
      "           1       0.89      0.80      0.84     23230\n",
      "\n",
      "    accuracy                           0.85     46415\n",
      "   macro avg       0.85      0.85      0.85     46415\n",
      "weighted avg       0.85      0.85      0.85     46415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, d2v_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ab829fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016955725519767317\n",
      "Actual\n",
      "0    0.724269\n",
      "1    0.275731\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "val_results = pd.DataFrame({\"Actual\": y_val,\n",
    "                           \"Word Embedding\": we_preds,\n",
    "                           \"1D CNN\": cnn_preds, \n",
    "                           \"Sentence Embedding\": d2v_preds})\n",
    "d2v_correct_over_others = val_results[(val_results[\"Sentence Embedding\"] != val_results[\"Word Embedding\"]) & \n",
    "           (val_results[\"Sentence Embedding\"] != val_results[\"1D CNN\"]) & \n",
    "           (val_results[\"Sentence Embedding\"] == val_results[\"Actual\"])]\n",
    "print(\"% of d2v predictions correct over other models:\\n\", round(d2v_correct_over_others.shape[0]/val_results.shape[0] * 100, 2)\n",
    "print(\"Class of d2v predictions that are correct over other models:\\n\", d2v_correct_over_others[\"Actual\"].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8a56a",
   "metadata": {},
   "source": [
    "Doc2Vec implementation is worse than Word2Vec models in all criterias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430fade",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba5edfc",
   "metadata": {},
   "source": [
    "Unable to incorporate tensorflow random forest/boosted trees model with tensorflow Sequential model, use LightGBM for runtime. \n",
    "Light GBM is different than traditional Gradient Boosted Trees in that it grows leaf first instead of level first, allowing it to train much faster.\n",
    "\n",
    "References: \n",
    "- https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n",
    "\n",
    "- https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html#deal-with-over-fitting\n",
    "\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/lightgbm-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02a952",
   "metadata": {},
   "source": [
    "## Avg Word Embeddings and Doc2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada751f4",
   "metadata": {},
   "source": [
    "Word embeddings are 2D arrays, take the average of the embeddings across each embedding index to flatten into 1D array, concatenate with 1D Doc2Vec embeddings in case DocwVec embeddings capture some information not already captured in the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original word embeddings\n",
    "with open('X_train_word_embedded.pkl', 'rb') as handle:\n",
    "    X_train_word_embedded = pickle.load(handle)\n",
    "with open('X_val_word_embedded.pkl', 'rb') as handle:\n",
    "    X_val_word_embedded = pickle.load(handle)\n",
    "with open('X_test_word_embedded.pkl', 'rb') as handle:\n",
    "    X_test_word_embedded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6057ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D CNN word embeddings\n",
    "with open('X_train_cnn_word_embedded.pkl', 'rb') as handle:\n",
    "    X_train_cnn_word_embedded = pickle.load(handle)\n",
    "with open('X_val_cnn_word_embedded.pkl', 'rb') as handle:\n",
    "    X_val_cnn_word_embedded = pickle.load(handle)\n",
    "with open('X_test_cnn_word_embedded.pkl', 'rb') as handle:\n",
    "    X_test_cnn_word_embedded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec embeddings\n",
    "with open('d2v_train.pkl', 'rb') as handle:\n",
    "    X_train_sent_embedded = pickle.load(handle)\n",
    "with open('d2v_val.pkl', 'rb') as handle:\n",
    "    X_val_sent_embedded = pickle.load(handle)\n",
    "with open('d2v_test.pkl', 'rb') as handle:\n",
    "    X_test_sent_embedded = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_train_word_embedded])\n",
    "X_val_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_val_word_embedded])\n",
    "X_test_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_test_word_embedded])\n",
    "\n",
    "X_train_concatenated = np.concatenate((X_train_word_embedded_avg, X_train_sent_embedded), axis = 1)\n",
    "X_val_concatenated = np.concatenate((X_val_word_embedded_avg, X_val_sent_embedded), axis = 1)\n",
    "X_test_concatenated = np.concatenate((X_test_word_embedded_avg, X_test_sent_embedded), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_train_cnn_word_embedded])\n",
    "X_val_cnn_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_val_cnn_word_embedded])\n",
    "X_test_cnn_word_embedded_avg = np.array([np.mean(i, axis = 0) for i in X_test_cnn_word_embedded])\n",
    "\n",
    "X_train_cnn_concatenated = np.concatenate((X_train_cnn_word_embedded_avg, X_train_sent_embedded), axis = 1)\n",
    "X_val_cnn_concatenated = np.concatenate((X_val_cnn_word_embedded_avg, X_val_sent_embedded), axis = 1)\n",
    "X_test_cnn_concatenated = np.concatenate((X_test_cnn_word_embedded_avg, X_test_sent_embedded), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_concatenated = np.concatenate((X_train_word_embedded_avg, X_train_cnn_word_embedded_avg, X_train_sent_embedded), axis = 1)\n",
    "X_val_all_concatenated = np.concatenate((X_val_word_embedded_avg, X_val_cnn_word_embedded_avg, X_val_sent_embedded), axis = 1)\n",
    "X_test_all_concatenated = np.concatenate((X_test_word_embedded_avg, X_test_cnn_word_embedded_avg, X_test_sent_embedded), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359fec30",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fced9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [2000] #Set to large number, set early stopping rounds\n",
    "learning_rate = [0.01] \n",
    "num_leaves = [15, 30, 50, 70] #tree grows leaf wise instead of depth wise for lightgbm, focus on tuning num leaves\n",
    "min_child_samples = [100] #set number of entries needed in a leaf to prevent overfitting, set to 100 since data is large\n",
    "reg_alpha = [0.1, 0.2, 0.3] #combined averaged word2vec embeddings with doc2vec embedding, so could be redundancy, set L1 regularization to drop out unnecessary features\n",
    "early_stopping_rounds = [50]\n",
    "random_state = [42]\n",
    "\n",
    "param_grid = {\"n_estimators\": n_estimators,\n",
    "              \"learning_rate\": learning_rate,\n",
    "              \"num_leaves\": num_leaves,\n",
    "              \"min_child_samples\": min_child_samples,\n",
    "              \"reg_alpha\": reg_alpha,\n",
    "              \"early_stopping_rounds\": early_stopping_rounds,\n",
    "              \"random_state\": random_state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "705de443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] Number of positive: 69639, number of negative: 69604\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 139243, number of used features: 200\n",
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500126 -> initscore=0.000503\n",
      "[LightGBM] [Info] Start training from score 0.000503\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[446]\tvalid_0's binary_logloss: 0.204129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'early_stopping_rounds': 50,\n",
       " 'learning_rate': 0.01,\n",
       " 'min_child_samples': 100,\n",
       " 'n_estimators': 2000,\n",
       " 'num_leaves': 50,\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': 0.3}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier()\n",
    "grid_search = GridSearchCV(estimator = lgb_model, param_grid = param_grid, cv = 5,\n",
    "                          n_jobs = -1, verbose = 2, scoring = \"f1\") \n",
    "grid_search.fit(X_train_concatenated, y_train, eval_set = [(X_val_concatenated, y_val)], eval_metric = \"logloss\")\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7bc7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0640b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('models_lgb.pkl', 'wb') as handle:\n",
    "#    pickle.dump(lgb_model, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model_cnn = lgb.LGBMClassifier()\n",
    "grid_search_cnn = GridSearchCV(estimator = lgb_model, param_grid = param_grid, cv = 5,\n",
    "                          n_jobs = -1, verbose = 2, scoring = \"f1\") \n",
    "grid_search_cnn.fit(X_train_cnn_concatenated, y_train, eval_set = [(X_val_cnn_concatenated, y_val)], eval_metric = \"logloss\")\n",
    "grid_search_cnn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c8124",
   "metadata": {},
   "source": [
    "## Word Embeddings Neural Network/1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2b508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8e3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3becd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
