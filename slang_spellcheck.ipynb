{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "863f2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "#import enchant\n",
    "#import neuspell\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c00138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Suicide_Detection.csv\")\n",
    "df = df[[\"text\", \"class\"]]\n",
    "df[\"text_cleaned\"] = df[\"text\"].str.lower().str.strip()\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].apply(lambda x: ''.join((c for c in unicodedata.normalize('NFD', x) if unicodedata.category(c) != 'Mn')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325861e",
   "metadata": {},
   "source": [
    "### Initialize SymSpell with dictionaries and define spell corrector function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113b6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit distance and prefix length set to default\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance = 2, prefix_length = 7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
    ")\n",
    "dictionary_path_bigram =  pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\"\n",
    ")\n",
    "\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(dictionary_path_bigram, term_index = 0, count_index = 2)\n",
    "\n",
    "def symspell_corrector(input_term):\n",
    "  # look up suggestions for multi-word input strings \n",
    "    suggestions = sym_spell.lookup_compound( \n",
    "      phrase=input_term,  \n",
    "      max_edit_distance=2,  \n",
    "      transfer_casing=True,  \n",
    "      ignore_term_with_digits=True, \n",
    "      ignore_non_words=True, \n",
    "      split_by_space=True \n",
    "  ) \n",
    "    return suggestions[0].term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b687dc58",
   "metadata": {},
   "source": [
    "### Convert Slang Words to Their Meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc9bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slang dictionary retrieved from https://floatcode.wordpress.com/2015/11/28/internet-slang-dataset/\n",
    "slang = pd.read_csv(\"slang_dict.csv\")\n",
    "slang = slang.dropna()\n",
    "#if there are multiple meanings for slang, meanings are separated by |, split and take first meaning \n",
    "slang[\"Meaning\"] = slang[\"Meaning\"].str.lower().str.split(\"|\").apply(lambda x: x[0])\n",
    "slang[\"Slang\"] = slang[\"Slang\"].str.lower()\n",
    "\n",
    "#Don't want to replace actual words if they double as slang, remove from slang df \n",
    "slang[\"Slang in Dict\"] = slang[\"Slang\"].apply(lambda x: True if \n",
    "                                              len(sym_spell.lookup(x, Verbosity.TOP, max_edit_distance = 0)) > 0 \n",
    "                                              else False)\n",
    "slang = slang[~slang[\"Slang in Dict\"]]\n",
    "#add word escapes for non-alphanumeric characters \n",
    "slang[\"Meaning\"] = slang[\"Meaning\"].apply(lambda x: re.escape(x))\n",
    "slang[\"Slang\"] = slang[\"Slang\"].apply(lambda x: re.escape(x))\n",
    "\n",
    "#add boundaries only for text that are solely composed of alphanumeric or space characters\n",
    "#does not convert when trying to add boundaries for text with nonalphanumeric/space characters, \n",
    "#keep them as is (replace all keys that contains nonalphanumeric/space characters) \n",
    "slang[\"only_alnum_space\"] = slang[\"Slang\"].str.contains(r'^[a-zA-Z0-9\\s]+$', regex = True)\n",
    "add_boundaries = slang[slang[\"only_alnum_space\"] == True]\n",
    "add_boundaries[\"Slang\"] = [rf'\\b{word}\\b' for word in add_boundaries[\"Slang\"]]\n",
    "no_boundaries = slang[slang[\"only_alnum_space\"] == False]\n",
    "slang = pd.concat([add_boundaries, no_boundaries])\n",
    "slang_to_meaning = dict(zip(slang[\"Slang\"], slang[\"Meaning\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae44d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing slang to meanings: 332 minutes\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "df[\"text_cleaned\"] = df[\"text_cleaned\"].replace(slang_to_meaning, regex = True)\n",
    "time2 = time.time()\n",
    "print(\"replacing slang to meanings:\", round((time2 - time1)/60), \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd3da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"len_text\"] = df[\"text\"].apply(lambda x: len(x))\n",
    "df[\"len_text_cleaned\"] = df[\"text_cleaned\"].apply(lambda x: len(x))\n",
    "#If cleaned text is 1.5 times longer than original text, most likely incorrect. \n",
    "#Greatest sources of error in converting is * to \"indicating spelling correction\" when it is used for emphasis\n",
    "#or converting websites http / www / .com \n",
    "df.loc[df[\"len_text_cleaned\"] / df[\"len_text\"] >= 1.5, \"text_cleaned\"] = \\\n",
    "df.loc[df[\"len_text_cleaned\"] / df[\"len_text\"] >= 1.5, \"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed9b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"Suicide Detection Slang Replaced.csv\")\n",
    "#df.to_pickle(\"Suicide Detection Slang Replaced.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840ac89",
   "metadata": {},
   "source": [
    "### Spellcheck Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a54fad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 0 rows in: 0 min\n",
      "Cleaned 10000 rows in: 9 min\n",
      "Cleaned 20000 rows in: 18 min\n",
      "Cleaned 30000 rows in: 28 min\n",
      "Cleaned 40000 rows in: 37 min\n",
      "Cleaned 50000 rows in: 47 min\n",
      "Cleaned 60000 rows in: 56 min\n",
      "Cleaned 70000 rows in: 66 min\n",
      "Cleaned 80000 rows in: 76 min\n",
      "Cleaned 90000 rows in: 86 min\n",
      "Cleaned 100000 rows in: 95 min\n",
      "Cleaned 110000 rows in: 104 min\n",
      "Cleaned 120000 rows in: 114 min\n",
      "Cleaned 130000 rows in: 123 min\n",
      "Cleaned 140000 rows in: 133 min\n",
      "Cleaned 150000 rows in: 142 min\n",
      "Cleaned 160000 rows in: 152 min\n",
      "Cleaned 170000 rows in: 161 min\n",
      "Cleaned 180000 rows in: 172 min\n",
      "Cleaned 190000 rows in: 185 min\n",
      "Unable to clean text at index: 197805\n",
      "Cleaned 200000 rows in: 199 min\n",
      "Cleaned 210000 rows in: 218 min\n",
      "Cleaned 220000 rows in: 232 min\n",
      "Cleaned 230000 rows in: 242 min\n",
      "cleaned text time: 244.323998717467 minutes\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "uncleaned = []\n",
    "for i in range(df.shape[0]):\n",
    "    if i%10000 == 0:\n",
    "        time2 = time.time()\n",
    "        print(f\"Cleaned {i} rows in: {round((time2 - time1)/60)} min\")\n",
    "    try:\n",
    "        df.loc[i, \"text_cleaned\"] = symspell_corrector(df.loc[i, \"text_cleaned\"])\n",
    "    except:\n",
    "        print(\"Unable to clean text at index:\", i)\n",
    "        uncleaned.append(i)\n",
    "        continue\n",
    "    \n",
    "time2 = time.time()\n",
    "print(\"cleaned text time:\", (time2 - time1)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da7713c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5오전5오전\\n5오전\\n5오전\\n5오전\\n5오전\\n5오전\\n5오전\\n5오전\\n5오전\\n5오전\\n아\\n아\\n차라리 살아보지 못한 편이 좋을거야'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only 1 row unable to be spellchecked at index 197805\n",
    "df.loc[197805, \"text_cleaned\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba9b1f",
   "metadata": {},
   "source": [
    "Only uncleaned text is in a foreign language, unable to decipher so leave as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffa43d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get new length of cleaned text\n",
    "df[\"len_text_cleaned\"] = df[\"text_cleaned\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "006d410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Spellchecked Suicide Detection.csv\")\n",
    "df.to_pickle(\"Spellchecked Suicide Detection.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39266529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
